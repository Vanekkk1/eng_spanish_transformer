{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xtPslmmO8P7v",
        "outputId": "c9ff3fbd-8122-4e1b-ed66-fb9f54b6e773"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSCOE5xk8P7x"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ii4Or4yd8P7y"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True\n",
        ")\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n",
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-UTDCe3v8P7y"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "vocab_size = 1000\n",
        "max_lenght = 50\n",
        "text_vectorization_eng = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, output_sequence_length=max_lenght\n",
        ")\n",
        "text_vectorization_spain = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, output_sequence_length=max_lenght\n",
        ")\n",
        "\n",
        "text_vectorization_eng.adapt(sentences_en)\n",
        "text_vectorization_spain.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PXkwT6Co8P7z"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:105000])\n",
        "X_valid = tf.constant(sentences_en[105000:])\n",
        "\n",
        "# Convert generator expressions to lists and then to tensors\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:105000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[105000:]])\n",
        "\n",
        "# Now vectorize the modified sentences\n",
        "y_train = text_vectorization_spain([f\"{s} endofseq\" for s in sentences_es[:105000]])\n",
        "y_valid = text_vectorization_spain([f\"{s} endofseq\" for s in sentences_es[105000:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqCIJnXZ8P7z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YBMq18l48P7z"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "num_stacks = 2\n",
        "num_heads_per_stack = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = embed_size\n",
        "\n",
        "\n",
        "# Define the model inputs\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "\n",
        "# Apply the TextVectorization layer\n",
        "encoder_input_vector = text_vectorization_eng(encoder_inputs)\n",
        "decoder_input_vector = text_vectorization_spain(decoder_inputs)\n",
        "\n",
        "# Define the shared embedding layer\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_size, output_dim=embed_size, mask_zero=True\n",
        ")\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_size, output_dim=embed_size, mask_zero=True\n",
        ")\n",
        "\n",
        "encoder_embedding = encoder_embedding_layer(encoder_input_vector)\n",
        "decoder_embedding = decoder_embedding_layer(decoder_input_vector)\n",
        "\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length), 2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]\n",
        "\n",
        "\n",
        "# Define and apply the positional encoding layer\n",
        "# positional_encoding_layer = keras_nlp.layers.SinePositionEncoding()\n",
        "pos_embed_layer = PositionalEncoding(max_lenght, embed_size)\n",
        "\n",
        "encoder_in = pos_embed_layer(encoder_embedding)\n",
        "decoder_in = pos_embed_layer(decoder_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe6Nu9GL8P70"
      },
      "source": [
        "# Encoder&Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kvNL05-c8P70"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "Z = encoder_in\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_vector, 0)[:, tf.newaxis]\n",
        "for _ in range(num_stacks):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "\n",
        "# Decoder\n",
        "encoder_outputs = Z\n",
        "Z = decoder_in\n",
        "decoder_pad_mask = tf.math.not_equal(decoder_input_vector, 0)[:, tf.newaxis]\n",
        "batch_max_len_dec = tf.shape(decoder_embedding)[1]\n",
        "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0\n",
        ")\n",
        "for _ in range(num_stacks):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=decoder_pad_mask & causal_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    cross_attentin_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    # key and value from encoder compared to decoder query\n",
        "    Z = cross_attentin_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "\n",
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9zz3ief8P70",
        "outputId": "4c0026a1-4529-473f-e618-e746f412049b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3282/3282 [==============================] - 1180s 357ms/step - loss: 2.9590 - accuracy: 0.4181 - val_loss: 2.2844 - val_accuracy: 0.5052\n",
            "Epoch 2/15\n",
            "3282/3282 [==============================] - 1150s 350ms/step - loss: 1.9464 - accuracy: 0.5651 - val_loss: 1.7006 - val_accuracy: 0.6114\n",
            "Epoch 3/15\n",
            "3282/3282 [==============================] - 1151s 351ms/step - loss: 1.6573 - accuracy: 0.6143 - val_loss: 1.5813 - val_accuracy: 0.6310\n",
            "Epoch 4/15\n",
            "3282/3282 [==============================] - 1172s 357ms/step - loss: 1.5328 - accuracy: 0.6360 - val_loss: 1.4550 - val_accuracy: 0.6556\n",
            "Epoch 5/15\n",
            "3282/3282 [==============================] - 1753s 534ms/step - loss: 1.4523 - accuracy: 0.6505 - val_loss: 1.3890 - val_accuracy: 0.6677\n",
            "Epoch 6/15\n",
            "3282/3282 [==============================] - 1153s 351ms/step - loss: 1.3949 - accuracy: 0.6613 - val_loss: 1.3567 - val_accuracy: 0.6722\n",
            "Epoch 7/15\n",
            "3282/3282 [==============================] - 1139s 347ms/step - loss: 1.3555 - accuracy: 0.6676 - val_loss: 1.3256 - val_accuracy: 0.6778\n",
            "Epoch 8/15\n",
            "3282/3282 [==============================] - 1142s 348ms/step - loss: 1.3190 - accuracy: 0.6747 - val_loss: 1.3058 - val_accuracy: 0.6817\n",
            "Epoch 9/15\n",
            "3282/3282 [==============================] - 1148s 350ms/step - loss: 1.2911 - accuracy: 0.6795 - val_loss: 1.2620 - val_accuracy: 0.6922\n",
            "Epoch 10/15\n",
            "3282/3282 [==============================] - 2232s 680ms/step - loss: 1.2725 - accuracy: 0.6830 - val_loss: 1.2564 - val_accuracy: 0.6928\n",
            "Epoch 11/15\n",
            "3282/3282 [==============================] - 1230s 375ms/step - loss: 1.2473 - accuracy: 0.6877 - val_loss: 1.2427 - val_accuracy: 0.6962\n",
            "Epoch 12/15\n",
            "3282/3282 [==============================] - 1177s 359ms/step - loss: 1.2281 - accuracy: 0.6916 - val_loss: 1.2297 - val_accuracy: 0.6983\n",
            "Epoch 13/15\n",
            "3282/3282 [==============================] - 1163s 354ms/step - loss: 1.2171 - accuracy: 0.6930 - val_loss: 1.2202 - val_accuracy: 0.6987\n",
            "Epoch 14/15\n",
            "3282/3282 [==============================] - 1210s 369ms/step - loss: 1.1958 - accuracy: 0.6970 - val_loss: 1.2196 - val_accuracy: 0.7018\n",
            "Epoch 15/15\n",
            "3282/3282 [==============================] - 1193s 363ms/step - loss: 1.1820 - accuracy: 0.7000 - val_loss: 1.1863 - val_accuracy: 0.7078\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " text_vectorization_1 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " text_vectorization (TextVe  (None, 50)                   0         ['input_1[0][0]']             \n",
            " ctorization)                                                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_1[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLam  (None, 50)                   0         ['text_vectorization[0][0]']  \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 128)              128000    ['text_vectorization[0][0]']  \n",
            "                                                                                                  \n",
            " positional_encoding (Posit  (None, 50, 128)              0         ['embedding[0][0]',           \n",
            " ionalEncoding)                                                      'embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 1, 50)                0         ['tf.math.not_equal[0][0]']   \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 50, 128)              527488    ['positional_encoding[0][0]', \n",
            " iHeadAttention)                                                     'tf.__operators__.getitem[0][\n",
            "                                                                    0]',                          \n",
            "                                                                     'positional_encoding[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 50, 128)              0         ['multi_head_attention[0][0]',\n",
            "                                                                     'positional_encoding[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 50, 128)              256       ['add[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 50, 128)              16512     ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 50, 128)              16512     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 50, 128)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 50, 128)              0         ['dropout[0][0]',             \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 50, 128)              256       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLa  (3,)                         0         ['embedding_1[0][0]']         \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 50, 128)              527488    ['layer_normalization_1[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 50, 128)              0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpL  (None, 50)                   0         ['text_vectorization_1[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.ones (TFOpLambda)        (50, 50)                     0         ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]',                        \n",
            "                                                                     'tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 50, 128)              256       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 1, 50)                0         ['tf.math.not_equal_1[0][0]'] \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.linalg.band_part (TFOpL  (50, 50)                     0         ['tf.ones[0][0]']             \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 50, 128)              16512     ['layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.math.logical_and (TFOpL  (None, 50, 50)               0         ['tf.__operators__.getitem_1[0\n",
            " ambda)                                                             ][0]',                        \n",
            "                                                                     'tf.linalg.band_part[0][0]'] \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 50, 128)              16512     ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 50, 128)              527488    ['positional_encoding[1][0]', \n",
            " ltiHeadAttention)                                                   'tf.math.logical_and[0][0]', \n",
            "                                                                     'positional_encoding[1][0]'] \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 50, 128)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 50, 128)              0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'positional_encoding[1][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 50, 128)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 50, 128)              256       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 50, 128)              256       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 50, 128)              527488    ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 50, 128)              0         ['multi_head_attention_3[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 50, 128)              256       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 50, 128)              16512     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 50, 128)              16512     ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 50, 128)              0         ['dense_5[0][0]',             \n",
            "                                                                     'layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 50, 128)              256       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.logical_and_1 (TFO  (None, 50, 50)               0         ['tf.__operators__.getitem_1[0\n",
            " pLambda)                                                           ][0]',                        \n",
            "                                                                     'tf.linalg.band_part[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 50, 128)              527488    ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.math.logical_and_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 50, 128)              0         ['multi_head_attention_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 50, 128)              256       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 50, 128)              527488    ['layer_normalization_7[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 50, 128)              0         ['multi_head_attention_5[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 50, 128)              256       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 50, 128)              16512     ['layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 50, 128)              16512     ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 50, 128)              0         ['dense_7[0][0]',             \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 50, 128)              256       ['add_9[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 50, 1000)             129000    ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3684584 (14.06 MB)\n",
            "Trainable params: 3684584 (14.06 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "early_st = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=3, monitor=\"val_accuracy\", restore_best_weights=True\n",
        ")\n",
        "model.fit(\n",
        "    (X_train, X_train_dec),\n",
        "    y_train,\n",
        "    epochs=15,\n",
        "    validation_data=((X_valid, X_valid_dec), y_valid),\n",
        "    callbacks=[early_st],\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-fAv8vxNHJgb"
      },
      "outputs": [],
      "source": [
        "model.save(\"eng_esp_model.keras\")\n",
        "model.save_weights('eng_esp_weights')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
