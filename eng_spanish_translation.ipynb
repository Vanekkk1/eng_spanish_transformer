{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xtPslmmO8P7v",
        "outputId": "c9ff3fbd-8122-4e1b-ed66-fb9f54b6e773"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSCOE5xk8P7x"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ii4Or4yd8P7y"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True\n",
        ")\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n",
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-UTDCe3v8P7y"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "vocab_size = 1000\n",
        "max_lenght = 50\n",
        "text_vectorization_eng = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, output_sequence_length=max_lenght\n",
        ")\n",
        "text_vectorization_spain = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, output_sequence_length=max_lenght\n",
        ")\n",
        "\n",
        "text_vectorization_eng.adapt(sentences_en)\n",
        "text_vectorization_spain.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PXkwT6Co8P7z"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:105000])\n",
        "X_valid = tf.constant(sentences_en[105000:])\n",
        "\n",
        "# Convert generator expressions to lists and then to tensors\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:105000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[105000:]])\n",
        "\n",
        "# Now vectorize the modified sentences\n",
        "y_train = text_vectorization_spain([f\"{s} endofseq\" for s in sentences_es[:105000]])\n",
        "y_valid = text_vectorization_spain([f\"{s} endofseq\" for s in sentences_es[105000:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqCIJnXZ8P7z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YBMq18l48P7z"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "num_stacks = 2\n",
        "num_heads_per_stack = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = embed_size\n",
        "\n",
        "\n",
        "# Define the model inputs\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "\n",
        "# Apply the TextVectorization layer\n",
        "encoder_input_vector = text_vectorization_eng(encoder_inputs)\n",
        "decoder_input_vector = text_vectorization_spain(decoder_inputs)\n",
        "\n",
        "# Define the shared embedding layer\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_size, output_dim=embed_size, mask_zero=True\n",
        ")\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_size, output_dim=embed_size, mask_zero=True\n",
        ")\n",
        "\n",
        "encoder_embedding = encoder_embedding_layer(encoder_input_vector)\n",
        "decoder_embedding = decoder_embedding_layer(decoder_input_vector)\n",
        "\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length), 2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]\n",
        "\n",
        "\n",
        "# Define and apply the positional encoding layer\n",
        "# positional_encoding_layer = keras_nlp.layers.SinePositionEncoding()\n",
        "pos_embed_layer = PositionalEncoding(max_lenght, embed_size)\n",
        "\n",
        "encoder_in = pos_embed_layer(encoder_embedding)\n",
        "decoder_in = pos_embed_layer(decoder_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe6Nu9GL8P70"
      },
      "source": [
        "# Encoder&Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kvNL05-c8P70"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "Z = encoder_in\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_vector, 0)[:, tf.newaxis]\n",
        "for _ in range(num_stacks):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "\n",
        "# Decoder\n",
        "encoder_outputs = Z\n",
        "Z = decoder_in\n",
        "decoder_pad_mask = tf.math.not_equal(decoder_input_vector, 0)[:, tf.newaxis]\n",
        "batch_max_len_dec = tf.shape(decoder_embedding)[1]\n",
        "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0\n",
        ")\n",
        "for _ in range(num_stacks):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=decoder_pad_mask & causal_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    cross_attentin_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads_per_stack, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    # key and value from encoder compared to decoder query\n",
        "    Z = cross_attentin_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "\n",
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9zz3ief8P70",
        "outputId": "4c0026a1-4529-473f-e618-e746f412049b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3282/3282 [==============================] - 1180s 357ms/step - loss: 2.9590 - accuracy: 0.4181 - val_loss: 2.2844 - val_accuracy: 0.5052\n",
            "Epoch 2/15\n",
            "3282/3282 [==============================] - 1150s 350ms/step - loss: 1.9464 - accuracy: 0.5651 - val_loss: 1.7006 - val_accuracy: 0.6114\n",
            "Epoch 3/15\n",
            "2180/3282 [==================>...........] - ETA: 6:06 - loss: 1.6816 - accuracy: 0.6103"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "early_st = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=3, monitor=\"val_accuracy\", restore_best_weights=True\n",
        ")\n",
        "model.fit(\n",
        "    (X_train, X_train_dec),\n",
        "    y_train,\n",
        "    epochs=15,\n",
        "    validation_data=((X_valid, X_valid_dec), y_valid),\n",
        "    callbacks=[early_st],\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fAv8vxNHJgb"
      },
      "outputs": [],
      "source": [
        "model.save(\"eng_esp_model.keras\")>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
